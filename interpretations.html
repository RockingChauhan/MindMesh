<!DOCTYPE html>
<html lang="en" dir="ltr">

<!-- Mirrored from dlprojectsrms.w3spaces.com/interpretations.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 11 Oct 2022 17:07:50 GMT -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Interpretations</title>
  <link rel="stylesheet" href="css/swiper-bundle.min.css">
  <link rel="stylesheet" href="css/style.css">

</head>
<body>

  <header>
    <div class="nav-bar">
      <a href="index.html" class="logo">Deep Learning</a>
      
      <div class="navigation">
        <div class="nav-items">
          <i class="uil uil-times nav-close-btn"></i>
          <a href="index.html"><i class="uil uil-home"></i> Home</a>
          <a href="explore.html"><i class="uil uil-compass"></i> Explore</a>
          <a href="about.html"><i class="uil uil-info-circle"></i> About</a>
          <a href="overview.html"><i class="uil uil-envelope"></i>Overview</a>
          <a href="history.html"><i class="uil uil-document-layout-left"></i> History</a>
          <a href="application.html"><i class="uil uil-envelope"></i>Applications</a>
        </div>
      </div>
      <i class="uil uil-apps nav-menu-btn"></i>
    </div>
  </header>
  <section class="home">
    <div class="swiper bg-slider">
      <div class="swiper-wrapper">
        <div class="swiper-slide">
          <img src="images/dp8.png" alt="">
        </div>
      </div>
    </div>
  </section>
  <section class="about section">
    <br>
    <center>
      <h2>Interpretations</h2>
    </center>
     <p>The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. 
         In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. 
         Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.</p>

    <p>The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow, proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function.
         If the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.</p>
      
    <p>The probabilistic interpretation derives from the field of machine learning. 
        It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.
         The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.
         The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.</p>
  </section>
  <div class="footer"> 
    Developed and designed by Sudhanshu Chauhan
  </div>
</body>

<!-- Mirrored from dlprojectsrms.w3spaces.com/interpretations.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 11 Oct 2022 17:07:50 GMT -->
</html>